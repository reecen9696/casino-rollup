# **Solana Coin Flip Betting MVP – Implementation Blueprint**

We’re building an **MVP**: a minimal, high-throughput coin-flip betting system on Solana that delivers **sub-second UX off-chain** and **zk-verified batch settlement on-chain**. A single operator runs a sequencer that takes signed bets, returns outcomes instantly, aggregates bet arithmetic into a Groth16 proof, and settles per-user vault PDAs on L1. This design ships fast, proves accounting correctness on-chain, and leaves clean seams to add in-circuit VRF, real deposits/withdraws, DA, and trustless exits later—without bloating the core.

**Functional requirements (MVP)**

- **Scope:** single game (coin flip), single operator; SOL/USDC balances *accounted* (deposits/withdraws mocked initially).
- **Performance:** p50 < **150 ms**, p95 < **300 ms** from bet → outcome (local); handle 1k-bet bursts; batch proving target **seconds**, on-chain verify **<300k CU**.
- **APIs:** POST /v1/bet {user, amount, guess, nonce, sig} → {bet_id, outcome, payout, status}; GET /v1/bets/:id; GET /v1/balance/:user; WS /ws for bet_result & batch_finalized.
- **Sequencer:** Axum service with idempotent bet_id, basic rate-limit, in-mem ledger (+ SQLite optional) and a 3–5s batcher → prover → verify_and_settle tx; crash-safe queue & dedup on resend.
- **Randomness (fairness):** CSPRNG first; upgrade path to **off-chain VRF** exposing {msg, sig, pubkey} per bet; client lib can verify.
- **ZK (accounting-only v0):** Groth16 circuit enforces per-bet win/lose booleans, balance deltas, conservation; public inputs = initial/final balances (+ batch_id).
- **On-chain programs:** Anchor **verifier** (verify_and_settle) + **vault** (PDA {owner, sol_balance, usdc_balance}); pre-check on-chain initial == pubInput.initial; post-write == pubInput.final; emit batch events.
- **Observability:** explorer stub lists bets/batches; logs include rng seed/VRF tuple; program events indexed.
- **Safety rails:** admin-gated settlement; strict PDA seeds; no double-spend; retries idempotent; stale initial balances cause safe failure.
- **Out of scope (for MVP):** real token transfers, multi-operator, trustless exits, heavy DA (keep a seam; add later if needed).

# **Phase 0 — Foundations**

1. toolchain + repos
    
    **Goal:** clean mono-repo + local env.
    
    **Build:** Rust stable, Solana CLI, Anchor, Node/PNPM; workspace with programs/ (vault, verifier), sequencer/ (Axum), prover/ (Arkworks circuit v0), explorer/ (tiny web).
    
    **Test:** anchor test runs, local validator boots, hello-world ix passes.
    
    **Exit:** CI green on lint/build/tests.
    

# **Phase 1 — Fast off-chain Coinflip (no chain, no ZK)**

1. sequencer API v0
    
    **Goal:** sub-second UX loop.
    
    **Build:** Axum REST POST /v1/bet, WS /ws; in-mem ledger + SQLite; CSPRNG outcome; balance checks; idempotent bet_id; basic rate limit.
    
    **Test:** 1k bets burst locally; p50 <150ms, p95 <300ms; no double-spend; restart → no bet loss.
    
    **Exit:** UX path proven fast & stable.
    
2. explorer stub
    
    **Goal:** see bets.
    
    **Build:** simple web listing bets from SQLite; filters by user, time; show outcome/payout.
    
    **Test:** matches sequencer logs; pagination OK.
    
    **Exit:** ops visibility.
    

# **Phase 2 — On-chain skeleton (no real settlement)**

1. programs scaffold
    
    **Goal:** wire Solana path end-to-end.
    
    **Build:** Anchor verifier program with verify_and_settle (stub: only emits event); Anchor vault program (structs only).
    
    **Test:** localnet tx hits verify_and_settle; events visible; program constraints tight.
    
    **Exit:** sequencer can submit a tx (placeholder).
    

# **Phase 3 — ZK minimal (accounting-only)**

1. circuit v0 (Arkworks Groth16)
    
    **Goal:** prove batch arithmetic (no sigs/VRF yet).
    
    **Build:** R1CS: for N bets, enforce win/lose boolean, Δbalances per bet, conservation, public inputs = initial/final balances (+ batch_id). Prover/verifier (off-chain).
    
    **Test:** valid batch → proof verifies off-chain; tamper any bet → verify fails.
    
    **Exit:** reproducible proofs.
    
2. on-chain Groth16 verify
    
    **Goal:** verify proofs on Solana.
    
    **Build:** embed VK; BN254 pairing syscall; verify_and_settle checks proof then (for now) updates in-program “LedgerAccount” PDAs (not real vaults).
    
    **Test:** send real proof → pass; mutated proof → ix fails; compute units < 300k.
    
    **Exit:** L1 verification solid.
    
3. sequencer→prover→L1 pipe
    
    **Goal:** real batch finalize.
    
    **Build:** sequencer batches every ~3–5s (or M bets), builds witness, proves, submits ix with accounts + pubInputs. Crash-safe queue & retries.
    
    **Test:** multiple batches finalize; DB reconciles with on-chain ledger; re-submit deduped.
    
    **Exit:** end-to-end correctness.
    

# **Phase 4 — VRF + fairness plumbing**

1. VRF output + auditability
    
    **Goal:** unbiased, auditable RNG (off-circuit for MVP).
    
    **Build:** ed25519-dalek keypair; per-bet input = H(bet_id||user||nonce); outcome = LSB(VRF_sign(msg)); store (msg, sig, pubkey) with bet; expose in API/Explorer.
    
    **Test:** client lib verifies sig; distribution ≈ fair over 1e6 sims; replay protection via user nonce.
    
    **Exit:** verifiable randomness (off-chain).
    

(keep circuit v0 as accounting-only; plan circuit v1 later to verify VRF/signatures in-circuit or move to SP1.)

# **Phase 5 — Real vaults (per-user PDA), still MVP deposits mocked**

1. vault PDAs (SOL + USDC)
    
    **Goal:** per-wallet vault accounts (design parity), deposits mocked.
    
    **Build:** Vault { owner, sol_balance, usdc_balance } PDA = [“vault”, owner]; SPL ATA for USDC owned by vault PDA; deposit_*/withdraw_* ix implemented but **disabled** behind admin flag for now; settle_delta ix internal.
    
    **Test:** init vault; invariants; owner mismatch rejected; ATA derivations correct.
    
    **Exit:** account model ready.
    
2. switch settlement to vaults
    
    **Goal:** write real balances.
    
    **Build:** verify_and_settle now writes user & house vault PDAs; pre-check on-chain initial == pubInput.initial; post-write equals pubInput.final.
    
    **Test:** after settle, RPC fetch vaults == off-chain; wrong initial → ix fails.
    
    **Exit:** L1 state = truth.
    

# **Phase 6 — Data availability & explorer v1**

1. DA publish
    
    **Goal:** reconstructability.
    
    **Build:** write batch JSON (inputs/outputs/balances) to IPFS/Arweave; include content hash/ID in proof public inputs and emit in event.
    
    **Test:** independent script downloads blob, recomputes totals; hash matches on-chain.
    
    **Exit:** public audit trail.
    
2. explorer on chain
    
    **Goal:** user-facing transparency.
    
    **Build:** index program events, show batches; per-bet page shows VRF tuple, timestamp, wager, outcome, before/after balances, DA link, L1 tx.
    
    **Test:** cross-checks with DA & RPC; handles pagination; realtime via websockets.
    
    **Exit:** production-grade visibility.
    

# **Phase 7 — Hardening & perf**

1. latency & throughput bench
    
    **Goal:** keep UX <250ms; stable batches.
    
    **Build:** load tool to fire 1–5k concurrent bets; measure p50/p95 end-to-end; prove time vs batch size curve; CU usage tracker on-chain.
    
    **Test:** target p95 <300ms for sequencer response; choose batch size that keeps prove < few seconds; verify CU headroom.
    
    **Exit:** tuned knobs documented.
    
2. failure & recovery
    
    **Goal:** idempotency & safety.
    
    **Build:** simulate: prover crash mid-batch, RPC failure, duplicate submissions, stale initial balances.
    
    **Test:** no funds mismatch; retries converge; stale initial → fails safely & auto rebuild.
    
    **Exit:** resilient ops.
    

# **Phase 8 — “seams ready” for SaaS + future trustless flows**

1. operator seams (not used yet)
    
    **Goal:** later multi-tenant.
    
    **Build:** add operator_id to batch meta; house vault per operator; config registry PDA; access control in verify ix.
    
    **Test:** multiple configs on localnet; only matching operator can settle its vaults.
    
    **Exit:** SaaS seam baked in.
    
2. design docs: trustless deposit/withdraw (later)
    
    **Goal:** path to full non-custodial exits.
    
    **Build:** spec two options: (A) Merkle state root + on-chain Merkle-verify withdraw; (B) “withdraw intents” proved in batch. Define circuit v1 deltas & program ix.
    
    **Test:** paper tests & small PoC Merkle proof on-chain.
    
    **Exit:** clear upgrade plan.
    

---

## **What to implement & test first (tight loop)**

1. Phase 1 (sequencer + UX speed)
2. Phase 3–6 minimal happy path: circuit v0 → on-chain verify → write to temp ledger → then flip to vault PDAs
3. Phase 4 (VRF auditability)
4. Phase 6 (DA + explorer)
5. Phase 7 (perf & failure)

---

## **Minimal APIs (MVP)**

- POST /v1/bet → {user, amount, guess, nonce, sig} → {bet_id, outcome, payout, vrf:{msg,sig,pub}, status:"pending"}
- GET /v1/bets/:id → status, batch, L1 tx
- GET /v1/balance/:user → off-chain view
- WS /ws → bet_result, batch_finalized
- On-chain: verify_and_settle(accounts[], proof, pub_inputs); (admin) init_vault, deposit_*/withdraw_* (disabled initially)

---

## **Acceptance gates (ship checklist)**

- Sub-second response (p95 <300ms) with 100+ concurrent bets
- Groth16 proof verifies on-chain; mutated proof rejected
- Vault PDA balances exactly match off-chain after settle
- DA hash in event; external replayer reproduces batch totals
- Crash/restart → zero data loss; retries idempotent

---

## **Risks to watch**

- Proof integration bugs (bind pubInputs to exact accounts; always pre-check initial balances)
- CU spikes (keep verify code lean; request CU budget)
- VRF censorship (log every received bet; expose feed; later add forced-inclusion path)
- Batch size vs proving time (measure early; tune)

That’s the build order. If you want, I can generate the repo scaffolding (folders, Anchor account structs, Axum routes, Arkworks circuit skeleton) next.

## **1. Infrastructure Layout (On-chain Programs, Sequencer, ZK Prover, DA, Explorer)**

**On-Chain (Solana L1):** The MVP utilizes two Anchor-based Solana programs – a **Vault Program** for managing user funds and a **Verifier Program** for validating zero-knowledge proofs. Each user has a dedicated **vault account** (a PDA derived from their wallet) to hold deposits in SOL and SPL tokens (e.g. USDC). Using PDAs ensures deterministic addresses under program control , meaning the program exclusively governs vault funds. The house (operator) also maintains its own vault (for bankroll), acting as counterparty to user bets. All vault accounts (including the house’s) are owned by the Vault Program, which enforces deposit/withdrawal rules and keeps account balances. The Verifier Program (or module) stores the SNARK **verifying key** and a compact representation of the game state (e.g. a Merkle root or batch ID). Initially, state is tracked per vault account (for simplicity, no global state root yet – “no Merkle exits yet” as noted). In future, the state could be *compressed* to a single on-chain Merkle root, but in the MVP each vault’s balance is individually updated on-chain after proofs .

**Off-Chain Sequencer:** A centralized **sequencer** service processes bets in real-time (sub-second latency). Users send bet requests to the sequencer (e.g. via a REST or WebSocket API). The sequencer holds the game logic: it receives user-signed bet instructions, checks the user’s off-chain balance (mirroring the on-chain deposit), and generates a random coin flip outcome. It then immediately returns the outcome to the user (for a fast, <1s response) and updates its local state of user balances (crediting wins or debiting losses). The sequencer batches multiple bets over a short interval and produces a **zero-knowledge proof** attesting to all bet outcomes’ correctness. This proof aggregates all the off-chain transactions (coin flips) into one succinct validity proof  . The sequencer, acting as rollup operator, periodically submits a **batch proof transaction** to the Verifier Program on Solana. Because many bets are verified in one proof, the on-chain cost is amortized – the Solana network only needs to verify a single SNARK instead of each bet individually . Initially, we target small batches (e.g. 10–100 bets) to fit Solana TX account limits when updating individual vaults.

**Zero-Knowledge Prover:** Off-chain, the sequencer runs a **ZK prover** engine to generate SNARK proofs of correctness for each batch of bets. The prover takes as input the initial state (user and house balances before bets) and all bet details (user inputs, random outcomes, resulting balances), then produces a succinct Groth16 proof that these outcomes were computed fairly and that no funds were created or destroyed improperly. Using a zk-SNARK ensures the proof remains small and fast to verify on-chain . Groth16 proofs are only 3 group elements and verify with just a few pairing checks , making them ideal for this rollup approach. The Solana runtime has built-in syscalls for BN254 pairings, allowing SNARK verification in under 200k compute units . (We compare specific proving technologies later in **Section 5**.)

**Data Availability (DA) Layer:** To ensure anyone can reconstruct the rollup state (bets and outcomes), the system must make raw transaction data available. In this MVP, we use a simplified approach: the sequencer can log or upload batch data to an off-chain store (e.g. Arweave or IPFS) for permanence. For example, an Arweave **uploader** service (using a Rust library like *Arloader*) can bundle each batch’s details (user bets, outcomes, new balances) and store it in the permaweb . A content link or hash to this data can be included in the proof’s submission transaction or an off-chain index. In the future, a more robust DA solution (like Celestia or Solana’s native account compression) can be integrated. The current approach is a placeholder – data is assumed available (or the operator is trusted not to withhold it) given the single-operator model.

**Explorer and Monitoring:** An **Explorer** service will track the L2 coin-flip activity and proof submissions. It can be a simple web dashboard or integrated into a block explorer. This service reads on-chain events (such as a **ProofVerified** event in the Verifier Program or state changes in vault PDAs) and correlates them with off-chain batch data from the DA layer. It should display: batch IDs, the list of bets in each batch (with anonymized user IDs), outcomes (win/lose), and changes in balances. This is akin to how zkSync or StarkNet explorers show L2 transactions and proofs  – for example, the explorer could show each coin flip bet as a virtual transaction with the result, and link to the L1 transaction that contains the proof covering that bet. The explorer will also highlight the proof verification status (pending, verified, or failed) for each batch. For developer debugging, the explorer can log performance metrics (e.g. proof generation time, verification time) and allow drill-down into any discrepancies between off-chain and on-chain state.

> Summary:
> 
> 
> **single operator**
> 

## **2. Anchor Program Schemas and Instructions (Vault and Verifier)**

### **Vault Program (User & House Vaults):**

**Account Schema:** The Vault Program defines a **Vault** account struct to hold a user’s balances and metadata. Each user wallet gets a dedicated PDA vault account, derived from a seed (e.g. ["vault", user_pubkey]) and the program ID, ensuring a unique address . The Vault account might store:

- owner: Pubkey (the user’s wallet, for reference),
- sol_balance: u64 (lamports deposited – optional, could also use the account’s lamport field),
- token_balance: u64 (balance of a specific SPL token like USDC, in minor units),
- optional space for multi-token support or a sequence number.

The house (operator) has a special vault (e.g. PDA with seed ["vault", house_pubkey] or a known constant seed like "house_vault"). The **house vault** holds the operator’s pool of funds used to pay out wins and collect losses. All vault PDAs have the Vault Program as owner and a designated **vault authority** (which can be the program or a delegate PDA) to sign transfers from the vault . Using a PDA authority allows the program to securely handle transfers of SOL/SPL on behalf of the vault (via signed invocations).

**SPL Token Handling:** For each supported SPL token, the program uses an associated token account (ATA) for the vault PDA. For example, the user’s vault PDA will own an ATA for USDC. Deposits of tokens are transferred into the vault’s ATA, and withdrawals are made from it. We can either maintain separate Vault records per token or extend the Vault account to include multiple token balances. In this MVP, we support one SPL token (e.g. USDC) plus native SOL for simplicity. The vault PDA itself can hold SOL (lamports) natively – when a user deposits SOL, lamports are transferred into the vault account (increasing its lamport balance). For tokens, the vault’s token ATA holds the actual tokens while the Vault account may store the token amount for quick reference and for use in proofs.

**Instructions (IX) in Vault Program:**

- **InitializeVault (optional):** Creates a new vault PDA for a user. In practice, this can be done implicitly on first deposit. The instruction would init a Vault Account<'info, Vault> with the user’s pubkey as a seed, payer = user, and allocate space for the Vault data . This sets initial balances to 0.
- **DepositSOL:** User sends lamports to their vault. This can be implemented as two steps in one transaction: (a) a system Transfer instruction moving lamports from the user’s wallet to the vault PDA (signed by the user), and (b) a call to the Vault program’s deposit_sol instruction to update the Vault account’s sol_balance. The program validates the transfer by checking the vault’s lamports or using AccountInfo::lamports() before vs. after. Alternatively, the program can invoke the system program’s transfer internally (with user as signer) . For MVP, we assume the deposit succeeds if the user provided the funds; exact lamport verification can be done by reading the account’s balance.
- **WithdrawSOL:** Transfers lamports from the vault PDA back to the user’s wallet. The instruction ensures the vault’s lamports >= requested amount and then invokes system_program::transfer from the vault PDA to the user (using the vault PDA signer seeds) . After transfer, it updates the vault’s record (sol_balance -= amount). (In reality, simply transferring lamports out will automatically reduce the account’s lamports, but we keep the sol_balance field in sync for proof purposes.)
- **DepositToken:** Similar to deposit SOL, but uses the SPL Token Program. The user must have an ATA for the token. The instruction checks that the user’s source token account and the vault’s token account are correct (mint matches, vault PDA is owner of vault token account). It then invokes spl_token::transfer from user ATA to vault’s ATA (user is signer) . The Vault account’s token_balance is incremented accordingly.
- **WithdrawToken:** Transfers tokens from the vault’s ATA back to the user’s ATA. The vault PDA (or its delegate) signs an spl_token::transfer from vault ATA to user ATA . The program decreases token_balance. (For safety, the program can enforce a cooldown or lock on withdrawals if needed, but for MVP, withdrawals are allowed after bets are settled.)
- **PlaceBet (optional/off-chain):** In this architecture, we do *not* have an on-chain place_bet instruction – bets are handled off-chain by the sequencer for speed. Thus, there is no direct on-chain instruction for a user bet. Instead, the user interacts with the sequencer, and the result is later reflected on-chain via the proof. (If we were to allow on-chain bets for fallback, a place_bet IX could lock the user’s funds and emit an event, but that defeats the purpose of sub-second off-chain betting.)

The Vault Program primarily manages storage and transfers. It should also expose a method to **apply batch updates** (only callable by the Verifier program or by a successful proof verification) – see **VerifyAndSettle** below.

### **Verifier Program:**

**Account Schema:** The Verifier Program holds the SNARK Verifying Key (VK) and minimal state needed for proofs. We define a **VerifierConfig** account (PDA) that contains:

- The Groth16 verifying key (in compressed form) or a reference to it,
- The current state root or a map of user balances commitments (for MVP, this might be empty or just an incrementing batch counter if each proof self-contains the balance updates),
- Perhaps an operator/admin pubkey for authorization (so only the sequencer can submit proofs initially).

Given that a Groth16 verifying key can be large (tens of KB), it could be stored in this account’s data. Alternatively, we compile the verifying key into the program as a constant (since it’s known at deploy time after circuit generation) . A third approach is to split the VK into multiple accounts if needed. For simplicity, assume the VerifierConfig holds the necessary parts of the VK or the program has them embedded (e.g. as constant bytes arrays) .

**Instructions (IX) in Verifier Program:**

- **VerifyAndSettle:** This is the core instruction that the sequencer calls to prove a batch of bets. It takes as input:
    - The accounts for all vaults involved in the batch (each user’s Vault account and the house Vault account, all as writable).
    - The VerifierConfig account (to get the verifying key).
    - An instruction data containing the SNARK proof (usually 192 bytes for proof + some for public inputs) and any public inputs (e.g. initial and final balances or state root hashes).
    
    The Verifier Program first **deserializes the proof and public inputs** from the instruction data (or accounts), then runs the SNARK verification. Using the groth16-solana crate, it will reconstruct the pairing check input and call alt_bn128_pairing via Solana syscalls  . If the proof is invalid, the instruction fails (reverts). If valid, the program proceeds to **settle the batch outcomes**:
    
    - It loops through each user vault account provided and updates its balance fields to the new values (which were proven in the SNARK to be correct). This could mean setting sol_balance = new_sol and token_balance = new_token for each user. The new values come from the public inputs of the proof (or are calculated by comparing old/new). The program must ensure the mapping of accounts to public input indices is correct (for example, the proof’s first public input might be the new balance of User A, etc.).
    - Simultaneously, it updates the house’s Vault account balance. The SNARK ensures conservation of funds: total debits equal total credits in the batch, so the house’s balance change equals the sum of wins/losses.
    - (If using a state root, this step would simply update a stored Merkle root instead of individual accounts. However, since we are directly updating accounts in MVP, we skip root updates.)
    
    **Account Validation:** Before applying updates, the program cross-checks that the on-chain accounts match the SNARK’s expected initial state. For each vault, it can compare the account’s current balance against the initial balance public input included in the proof. This guards against a scenario where an outdated proof is used or the accounts changed (e.g., via deposits) since the proof was generated. If any mismatch is found, the program throws an error (the proof would effectively be invalid for the current state). By enforcing this, we tie the proof to the exact state of each vault at batch start, achieving state consistency  .
    
    After verification and updates, the program might emit an event (e.g. BatchSettled { batch_id, num_bets, total_won, total_lost }). This helps the explorer/clients to know that batch settlement succeeded.
    
- **InitializeVerifierConfig:** (One-time setup) Creates the VerifierConfig account (PDA, e.g. seed ["verifier-config"]) with the admin and possibly writes the verifying key bytes into it. This would be called during deployment by the operator. It might also store the initial state root if we had one. Since our current design uses explicit vault accounts, the initial state root could be a hash of all user balances = just the house balance (since all users start with 0 until deposit). This is mainly to satisfy future rollup extension but not critical now.
- **SetVerifyingKey (optional):** Allows updating the verifying key (e.g. if we deploy a new circuit version). This would be restricted to the admin. In production, changing the circuit would require a coordinated trusted setup again (for Groth16) , so this is mostly for development.
- **EmergencyUnlock (optional):** If something goes wrong (e.g. sequencer goes down), an admin or governance could call an instruction to allow users to withdraw directly from their vaults without a proof. This effectively falls back to a custody contract. For MVP, we simply note this possibility; implementing it requires careful access control (perhaps a time-lock before it takes effect, giving the operator a chance to come back online).

**Cross-Program Invocation (CPI):** The Vault and Verifier programs could be separate, but they need to work together for **VerifyAndSettle**. Two architectures are possible:

1. **Single Program:** Merge Vault and Verifier logic into one Anchor program. This program holds vault accounts and also verifies proofs. After verifying, it directly updates its accounts. This is simpler for an MVP (no CPI needed). We would include the verifying key and verification code in the same program.
2. **Two Programs:** Keep Vault and Verifier separate for modularity. In this case, the Verifier program, upon successful proof, must invoke the Vault program to apply the balance changes. That could be done via a CPI call like VaultProgram::settle_batch(accounts, new_balances). However, this adds complexity and overhead. For MVP, we lean toward a single combined program, implementing vault functions and proof verification together for atomicity.

Either way, **only an authorized entity** (the sequencer) should be allowed to call the proof verification instruction. We can enforce this by requiring the signature of a specific *operator key* on VerifyAndSettle, or by making the sequencer send a signed transaction to call it. In a future multi-operator scenario, this might be replaced with a permissionless proof submission (anyone can submit a valid proof) combined with fraud proofs or slashing for invalid submissions – but with zk-SNARKs, an invalid proof will simply fail verification (no need for slashing as it can’t be forged except with negligible probability).

**Rent & Storage:** Each vault PDA will allocate enough space for the account data (say 8-byte discriminator + fields). The VerifierConfig stores the VK (~ maybe 1–2 KB if using compressed points, or more). The on-chain storage footprint grows with number of users (each with a vault account), but thanks to batch processing, the transaction load doesn’t grow per bet. Solana’s recent **state compression** solution showcases storing multiple accounts’ state in one Merkle tree with a single on-chain root . While we aren’t using the compressed account technique in this MVP, the concept aligns with future optimization: we could compress hundreds of vault balances into one 32-byte Merkle root, and have the proof update that root   instead of individual accounts, enabling much larger batches. For now, clarity and simplicity prevail – explicit accounts are easier to debug.

## **3. Coin Flip Logic – Randomness Generation and Verification**

**Randomness Challenges:** A fair coin flip requires an unpredictable outcome. Solana on-chain programs cannot directly generate randomness due to determinism – validators must produce the same result given the same state . Using values like recent blockhash or timestamp is insecure, as they can be influenced by validators or anticipated by users. Thus, **off-chain randomness** is needed . We solve this via a Verifiable Random Function (**VRF**) or cryptographic commit-reveal scheme run by the sequencer.

**Sequencer VRF Approach:** The sequencer service holds a VRF private key and public key. For each bet, the sequencer computes a random outcome by applying the VRF to a unique input (e.g. a combination of the bet ID and perhaps a global nonce). The VRF (for example, using an ed25519-based VRF on Solana ) produces:

- A random 64-bit number (or a bit, for coin toss),
- A proof (or signature) that this number was derived from the input and the sequencer’s secret key.

The sequencer can convert the VRF output into a coin flip result (e.g. take the least significant bit: 0 = heads, 1 = tails). The **proof of randomness** is retained so that the outcome can be verified. In our design, we have two levels of verification:

1. **Off-chain verification:** The user’s client (or any observer) can verify the VRF proof using the sequencer’s known public key to ensure the outcome wasn’t biased. This is similar to how oracles like Switchboard provide randomness – they publish an on-chain VRF value and anyone can verify the signature off-chain . For MVP, we might skip automatic client verification, but the data is there to audit fairness.
2. **On-chain (or in-circuit) verification:** The ideal scenario is to include the VRF proof in the zk-SNARK, so that *the proof itself attests that each bet outcome equals VRFsequencer(bet_id)*. This would fully eliminate trust in the sequencer for randomness. The SNARK circuit would need to validate the VRF’s elliptic-curve computations, or at least verify an ed25519 signature inside the circuit – a complex but feasible task. Projects like SP1 zkVM can accelerate ed25519 verification inside proofs . However, implementing VRF verification in-circuit for an MVP might be overly time-consuming, so we plan a compromise: trust the sequencer to generate VRF outputs honestly, but make all outputs and proofs public for anyone to verify externally. Essentially, the sequencer’s **reputation** is at stake – any attempt to cheat (e.g. by selectively withholding bets with unfavorable outcomes) could be detected by the community.

**Commit-Reveal Alternative:** Another strategy is a commit-reveal scheme: The sequencer could publish a commitment to a random seed (e.g. a hash of a random value) before accepting bets, then use that seed to derive outcomes, and later reveal the seed. However, in a continuous betting service, committing new seeds for each round and waiting for reveals adds latency. It’s simpler to use a VRF which provides immediate verifiable randomness per bet.

**Avoiding Sequencer Manipulation:** With a VRF, the sequencer cannot **bias** the coin toss after seeing the user’s bet – the output is determined by the secret key and input, and the proof ensures it was not arbitrarily chosen . One remaining risk is **censorship**: the sequencer might decide not to include a particular bet in any batch if the outcome was unfavorable (for instance, if a high-stakes user bet would win against the house). In a single-operator setup, this is a trust issue – users must somewhat trust the house not to censor winning bets. We mitigate this by transparency: if the sequencer generates an outcome and fails to report it on-chain, a user could complain or prove it (since they would have seen the outcome via WebSocket). In future, multiple operators or a turn-based VRF (where user contributes randomness too) could solve this. As an extension, we could let the user provide a client-generated random seed that is hashed and included in the bet, so the outcome is a mix of user and house randomness (preventing house from having sole control). The SNARK could then prove it combined both inputs (like using XOR or hash) for the final result.

**Integration with ZK Proof:** For MVP, the SNARK circuit will simply take the outcome as a given public input (or witness) without proving its randomness. The circuit will, however, prove that given those outcomes, the state updates are correct. We will log the VRF proofs separately. This reduces what’s proven on-chain but is far simpler initially. The design nonetheless keeps the door open to later incorporate full randomness verification in-circuit:

- We’d allocate circuit constraints to verify each outcome’s VRF signature against the sequencer’s public key (hardcoding the key in the circuit or passing as public input).
- SP1’s zkVM, for instance, can include ed25519 verification in its logic via specialized precompiles , making this more approachable than writing it in Arkworks constraints.

**Outcome Verification:** Even without in-circuit VRF proofs, the on-chain program can verify the *range* of outcomes. The circuit can constrain that each coin flip result is a boolean (0 or 1) – ensuring no weird values. It can also enforce a property like: if result = 0 (heads), then user’s balance change = +bet payout; if 1 (tails), balance change = –bet amount. This way, the SNARK guarantees the outcomes were applied consistently. The fairness of outcome generation is thereby the only aspect not proven on-chain in MVP, but all arithmetic and balance changes are provably correct.

**Using Oracles (Switchboard/Orao):** Alternatively, one could route randomness through an on-chain oracle network. For example, Switchboard and ORAO provide Solana VRF services . The program could request randomness from such an oracle and get a callback with a verified random value. However, that introduces on-chain latency (multiple transactions, likely >1.5s) which conflicts with our sub-second goal. It also complicates the zk-rollup design, since each bet would become at least one Solana transaction. Thus, we keep randomness generation off-chain for speed. If needed, the sequencer itself could *be* a Switchboard oracle – running a node that signs VRF values – effectively what we’re doing but in a custom setup.

**Summary:** Random outcomes are generated by the sequencer using a cryptographically secure method (VRF). The system is designed to minimize trust required: all random outputs carry proofs that can be verified outside the chain. In the ZK proof, we ensure outcomes are used correctly, and leave full on-chain randomness verification as a future enhancement. Using off-chain VRF aligns with Solana best practices (since on-chain programs can only consume randomness provided by an oracle) . With this approach, users can be confident the coin flips are not predictable by players or easily manipulable by the house.

## **4. ZK Circuit Design (Verifying Bets, Signatures, Outcomes, Vault Accounting)**

The heart of the system is the **zk-SNARK circuit** that attests to a batch of coin flip bets. We outline the circuit’s function and constraints in detail:

**Proving Statement:** “Given an initial state of user and house balances, and a set of signed bet instructions with random coin flip outcomes, the circuit computes the correct new balances for each party. The prover (sequencer) knows the secret inputs (the users’ signatures or hashed bets, and the random draws), and the output is a proof that (a) each bet was authorized, (b) the outcome was applied fairly, and (c) no funds were created/destroyed beyond the game rules, yielding the final state.” Importantly, the proof does not reveal any secret info (it doesn’t need to in this case – the outcomes aren’t secret; it’s a *validity proof* rather than a privacy proof).

**Circuit Inputs:**

- **Public Inputs:** These are values encoded into the proof that the verifier program will supply/verify. Likely public inputs include:
    - Each user’s initial balance and final balance (for all users in the batch, including the house). Alternatively, a commitment like a Merkle root representing these balances could be used, but in MVP we can list balances directly.
    - Perhaps the total number of bets or a batch ID (to ensure proofs can’t be replayed in a different context).
    - If we were verifying randomness, we might also include the VRF public key and some sort of hash of all VRF outputs as a public input, but for now that’s omitted.
- **Private (Witness) Inputs:**
    - The details of each bet: user ID (or index), bet amount, the user’s signature on the bet, and the VRF random output.
    - The secret randomness (like VRF private computations) if needed to verify it, or simply the resulting coin flip bit if we treat it as witness without proving.
    - The user’s private key is *not* needed – instead, we have the user’s signature which is checked using the user’s public key (which either is known to the circuit or given as part of bet data).
    - We assume the user’s public key is known (since it’s the vault owner or could be included in the bet data).

**Constraints & Verification inside Circuit:**

1. **Signature Verification:** For each bet, the circuit must ensure it was authorized by the user. We can use each user’s Solana public key (ed25519) and verify that the bet parameters (like “user, bet_amount, maybe a nonce/bet_id”) were signed by that key. Implementing ed25519 verification in R1CS is complex due to ed25519 using curve25519 (not friendly to BN254 field). However, it’s possible by either:
    - Using an elliptic curve cycle (e.g. Ristretto inside the circuit via custom constraints or an embedded gadget library), or
    - Using a zkVM approach (like SP1) which supports ed25519 via precompiled gadgets .
        
        For the MVP, we might simplify by not verifying user signatures in-circuit (and instead rely on the operator to only include real user bets). But it’s worth blueprinting: if we verify signatures, we’d include a constraint for each bet: verify_sig(user_pubkey, message_hash, signature) = true. The message could be a hash of the bet details (user, amount, a batch nonce, and maybe expected outcome = user’s guess if it were a prediction – but in coin flip the user typically just calls heads or tails; our game might be a fixed guess or always heads vs tails? We should clarify: possibly the user chooses heads or tails. If so, that guess must be part of the bet and the outcome comparison).
        
    - *If the user selects heads or tails:* Then the bet details include the user’s guess. The circuit would then check outcome vs guess to determine win/loss.
    - *If the game automatically considers one outcome a win (say heads means user wins the double, tails means house wins user’s bet):* Then user doesn’t guess – it’s purely luck. In that case, no input from user besides amount.
        
        We assume user guesses heads/tails in their signed message to make it more game-like. The circuit will verify signature on (bet amount, guess, maybe an ID).
        
2. **Random Outcome Application:** The circuit takes the random coin result (0 or 1) for each bet (provided as a witness or derived from witness input). It enforces that if the result = 0 (for example) and the user guessed 0 (heads), then the user wins; if guessed incorrectly, the user loses. The actual arithmetic implemented:
    - If user wins: new_user_balance = old_user_balance + payout, new_house_balance = old_house_balance - payout.
    - If user loses: new_user_balance = old_user_balance - bet_amount, new_house_balance = old_house_balance + bet_amount.
        
        The payout could be equal to bet_amount (if 2x payout means user gets their bet back plus equal amount from house, net gain = bet; many coin flips double the money on win). For simplicity, assume a 1:1 payout (win gives +bet, lose gives -bet).
        
        The circuit must enforce these updates. A convenient way is to use boolean selector constraints:
        
    - Let win = 1 if user won, or 0 if lost (this can be derived by comparing guess and outcome).
    - Then new_user_balance = old_user_balance + win * bet_amount - (1 - win) * bet_amount. This effectively adds bet_amount if win=1, or subtracts bet_amount if win=0 (loss). Similarly for house: new_house_balance = old_house_balance - win * bet_amount + (1 - win) * bet_amount.
        
        These are linear constraints suitable for SNARKs.
        
3. **Balance Non-Negativity:** The circuit should ensure no balance goes negative. That implies old_user_balance >= bet_amount for each bet (user had enough funds to wager). If not, that bet is invalid. We can constrain this by checking an inequality inside the circuit. Direct inequalities are tricky in SNARKs, but we can enforce via range checks (ensuring the subtraction has no overflow in field, or using bit decomposition to ensure no borrow). Alternatively, the program should prevent on-chain any bet bigger than deposit; since the sequencer controls that, we assume it won’t include invalid bets in proof (it would have nothing to gain by cheating because proof would fail if it tried to create negative balances on output that don’t match input minus bet).
4. **Conservation of Funds:** The circuit inherently maintains conservation by adjusting user and house balances complementarily. To be safe, one could add a constraint that the sum of all user balances + house balance remains constant from start to end of batch (assuming no external deposits during the batch). However, if multiple users are in a batch, one user’s loss is house’s gain (not other users’ gain, since it’s not player-vs-player). So total sum change = 0, which the circuit already enforces through each bet’s arithmetic. If we want, we can add a final check: sum(final_balances) == sum(initial_balances) as a public input equality, as a sanity check.
5. **Batch Processing:** The circuit needs to handle multiple bets in one proof. There are a couple of approaches:
    - **Fixed-size batch circuit:** e.g. support exactly N bets per proof. If fewer bets, some inputs are zeroed or we include dummy bets with 0 amount that don’t change state. This is simpler to implement in R1CS (fixed number of constraints).
    - **Dynamic/Recursive approach:** e.g. use a loop in a zkVM or do recursive proofs to aggregate flexible numbers of bets. SP1 zkVM would let us write a Rust loop over all bets, theoretically enabling a proof that scales to arbitrary count (bounded by proving time). For MVP, we might choose a fixed batch size like 8 or 16 bets per proof to simplify the Arkworks circuit. We can later compile a new circuit if we need a different batch size.
    
    If using a fixed batch circuit, the *public inputs* will include the initial and final balances for each account touched. The circuit will have to identify which user each bet corresponds to. We could simplify by only allowing one bet per user per batch, but that’s not necessary. Instead, we maintain a mapping:
    
    - The circuit can take a vector of user accounts (Pubkeys or just an index for each unique user in batch). Each bet then has an index pointing to the user in that vector. The circuit will update that user’s balance accordingly.
    - This adds complexity. Simpler: we could restrict one bet per user per batch for now, so the order of public inputs can align with bet order.
    - However, multiple bets per user per batch is fine since they could just be treated as separate bets sequentially updating that user’s balance. We just have to ensure the final balance reflects all of them.
    - A straightforward method: process bets sequentially *within the circuit*. The circuit can keep running totals of each user’s balance. E.g., it starts with variables for each user’s starting balance (from public input). For each bet, it finds the user and house balances, updates them. At the end, it outputs each user’s final balance (which must match the public inputs given for final state).
    - Because this is a lot of logic, using a high-level circuit language or zkVM is helpful. In Arkworks, we’d end up writing constraints that simulate a lookup for the right user balance (maybe using a select gadget across possible user indices, or just unrolling if small count).
6. **Outcome Randomness Check (optional):** If we were integrating VRF verification:
    - The circuit would take the VRF proof and public key (both known). It would replicate the VRF verification steps: essentially verifying an ed25519 signature on the hash of the input that produced the random output. This involves elliptic curve operations (point multiplications) and hashing (e.g. SHA-512 in ed25519). These are *heavy* in R1CS. We likely skip including this due to time.
    - Instead, as noted, we ensure each outcome is either 0 or 1 (enforce outcome * (outcome - 1) = 0 as a constraint, which is true only if outcome is 0 or 1 in a field of moderate size).
    - We could also incorporate a hashed transcript of all outcomes to tie them to a commitment the sequencer made (for example, a Merkle tree of outcomes, or simply the final house balance acts as an implicit commitment since if they cheated on outcomes, house balance wouldn’t match). Because the house balance at end is determined by all outcomes, the proof’s validity indirectly proves a consistent set of wins/losses added up to that balance.

**Circuit Output/Public Verification:** The public inputs are arranged so that the Solana verifier program can easily plug in on-chain values. For each vault account passed to the VerifyAndSettle instruction, the program knows the *initial* balance (from before the batch) and it expects a *final* balance (after batch). We can encode the public inputs as pairs of values for each account: e.g. [init_user1, init_user2, …, init_house, final_user1, final_user2, …, final_house]. The on-chain code can populate the initial values from account data and final values from what it will set. The SNARK verifies that these correspond to a valid sequence of bets. Once the proof passes, the program writes those final values into the accounts  .

**Proof-of-Concept Simplification:** To ensure we can implement this quickly, we might simplify the first circuit:

- Omit signature checks (assume bets are honest).
- Omit VRF checks (trust outcomes).
- The circuit then only proves **balance arithmetic correctness** given a set of predetermined win/lose outcomes. This is far easier to implement and still gives the crucial benefit: the operator cannot lie about the results or steal funds, because the proof will fail if the arithmetic doesn’t add up or if they try to credit themselves incorrectly . Users are still trusting the operator on the fairness of the random, but not on the accounting.

Even this simpler circuit ensures the house can’t, for example, deduct 2x the bet from a losing user or fail to pay a winning user – any deviation from the 1:1 payout rules would break a constraint. Also, the operator can’t include a bet for a user who didn’t actually have funds (since initial balance is public and on-chain visible) or fabricate a deposit.

**Circuit Implementation Strategy:** We have two main options for writing the circuit:

- **Arkworks / Handwritten R1CS:** Use the Arkworks libraries (ark-relations, ark-groth16 etc.) to create constraints as per above logic. We would define a custom struct CoinFlipCircuit that implements ConstraintSynthesizer<Fr> . In generate_constraints, we allocate variables for all inputs and enforce the equations (balance updates, etc.) for each bet. We then generate a proving key and verifying key for this circuit. This approach requires a new trusted setup each time we change the circuit (Groth16 circuit-specific setup) , but it gives us full control.
- **SP1 zkVM:** Use Succinct’s SP1 to write the logic in Rust code (which gets executed in a RISC-V-like VM and proven). We could write a Rust function that takes a list of bets and does the same math, using standard conditionals and loops. SP1’s toolchain will then generate a SNARK proof that the program executed correctly. The advantage is ease of development (no manual constraint writing) and built-in support for things like ed25519 or SHA if we need them via precompiles . The drawback is setting up the SP1 environment and the proof verifier on Solana. However, SP1 outputs a Groth16 proof on BN254 as well, and they provide a Solana verifier crate for those proofs , so integration is plausible. For MVP, Arkworks might be more straightforward if we’re already comfortable with Anchor + Rust, since SP1 is newer.

**Security of Circuit:** The circuit’s security relies on the SNARK’s soundness. Groth16 provides a high level of assurance (with a valid trusted setup). We must be careful that all critical game rules are enforced by constraints:

- For example, if we forgot a constraint to tie a user’s final balance variable into the public input properly, a malicious prover could prove a nonsense outcome that changes balance arbitrarily. So we will double-check constraints (and write tests to ensure an invalid state fails proof).
- The use of public inputs is careful: the verifying program will check these against on-chain data. For instance, if a user’s initial balance in proof doesn’t match the vault account lamports, the program will reject the proof. This binding between proof and on-chain state is crucial for correctness .

**Efficiency Considerations:** Each additional check (signature verify, VRF verify) can add many constraints. Groth16 proving time grows roughly linearly with number of constraints. For MVP, keeping constraints minimal helps achieve faster proving (aiming for a few seconds at most on a decent server). The verification time on-chain remains constant (a handful of pairings) regardless of circuit complexity . If we anticipate sub-second *verification* on Solana (which is fine – 200k CU ~ 0.2s), the heavy lift is *proving*. We will measure proving performance in testing. We may decide to limit batch sizes to what we can prove in, say, <1 second on a laptop (for rapid development) or a bit longer on better hardware. If needed, we can reduce N (bets per proof) or drop the expensive checks initially.

In summary, the ZK circuit ensures the off-chain sequencer follows the game rules. It’s effectively a **batch transaction validator** , analogous to an Ethereum rollup SNARK proving that a sequence of transactions was executed correctly . The constraints cover user authorization, game logic, and balance updates. The result is a proof small enough to verify on Solana’s L1, which then triggers on-chain state changes with confidence that they reflect valid coin flips.

## **5. Groth16 (Arkworks) vs. SP1 zkVM – Comparison for Use, Speed, and Solana Integration**

We consider two approaches to implement the ZK proving system: using a traditional circuit with **Arkworks & Groth16**, or using the **SP1 zkVM** by Succinct. Both ultimately rely on Groth16 proofs over the BN254 curve for on-chain verification, but they differ in developer experience and performance optimizations.

### **Arkworks Groth16:**

**Ease of Use:** Arkworks is a Rust ecosystem for zkSNARK programming . It provides low-level libraries to construct circuits (constraints) and perform Groth16 proving and verification. Using Arkworks means we manually encode the coin flip logic as constraints, which can be intricate for non-trivial operations (like ed25519 verification or loops). There is a learning curve – we’ll need to be comfortable with Rank-1 Constraint Systems (R1CS) and possibly utilize gadget libraries (like ark-r1cs-std for common operations). However, for basic arithmetic and boolean logic, Arkworks is quite direct. We’d use ark_groth16::generate_random_parameters to do a trusted setup and Groth16::prove to create proofs  . Arkworks gives fine-grained control, which is useful to optimize constraints count. The downside: any change to the business logic requires regenerating the circuit and a new trusted setup (Groth16 is circuit-specific ). Also, verifying an Arkworks proof directly in Solana requires using Solana’s BN254 syscalls; we cannot run the Arkworks verifier (which is designed for off-chain) inside a Solana program due to compute limits .

**Performance:** Groth16 proofs are extremely fast to **verify** (just a few pairings) , which is why they’re ideal for on-chain use. Solana’s BN254 verification via syscalls takes <200k compute units  – easily within a single transaction’s budget. **Proof generation** performance, however, depends on circuit complexity and proving algorithm implementation. Arkworks uses multi-threading and is optimized in Rust, but it might not be as bleeding-edge fast as some newer proving systems or assembly-optimized frameworks. Still, for our relatively small circuit (perhaps on the order of tens of thousands of constraints), Arkworks should produce proofs in seconds on a CPU. If we include heavy crypto (signature checks), constraint count could skyrocket, slowing proofs significantly. In such case, one might consider alternatives or optimizations (e.g. using smaller field curves or simplifying checks).

**Verifier Setup on Solana:** Using Arkworks means we have a custom circuit, and we’ll need to integrate the verifying key into our Solana program. Tools exist to convert a verifying key to Solana-friendly format. For instance, Light Protocol’s groth16-solana crate takes a Circom-generated verifying key JSON and outputs Rust code for verification . We can do similar with Arkworks: after generating the verifying key, we can serialize it and embed it. The on-chain verification then uses a routine (like Light Protocol’s) to construct the pairing input and call the syscall  . We have references like the example by wkennedy which shows how to feed proof data to Solana’s alt_bn128_pairing syscall  . Essentially, the integration is proven: *Arkworks + groth16 + solana-bn254 = working solution*, as demonstrated by projects implementing zk-rollups on Solana .

**Community & Resources:** Arkworks is widely used in Ethereum and other blockchain zk projects, so there are tutorials and a knowledge base. We can leverage existing examples (like the tutorial circuit we found  or Light Protocol’s circuits) as a starting point.

### **SP1 zkVM (Succinct):**

**Ease of Use:** SP1 is a high-performance zkVM allowing developers to write programs in Rust (even using std library and many crates) and then generate proofs for their execution . The allure is we can implement the coin flip logic in normal Rust code (for example, iterate through a list of bets, update variables) without manually worrying about constraints. This dramatically simplifies development – complex tasks like signature verification or hashing can be done by calling libraries or using SP1’s precompiles. SP1 supports custom precompiled gadgets for common operations such as **ed25519 signature verify, secp256k1, SHA-256, Keccak** etc., providing *5-10x speedups* in those operations compared to naive circuit code  . For instance, SP1’s built-in support could let us verify ed25519 signatures for each bet far more easily than implementing ed25519 in Arkworks.

Using SP1 involves its own toolchain (based on RISC-V and STARK/Plonk proofs internally). We would:

- Write the coin flip logic as an SP1 program (a Rust function in a no_std environment possibly).
- Use cargo prove to generate a proof (SP1 will do a STARK proof internally and then wrap it in a Groth16 proof for on-chain).
- Deploy an on-chain verifier. SP1 provides an SDK to generate an on-chain verifier (Solidity for EVM, and via sp1-solana for Solana) . The verifier is essentially the Groth16 verification like any other, but tied to SP1’s fixed verifying key (or one generated for that program).

One complexity: SP1 still needs a proving key setup for each program, although they mention being feature-complete and possibly have a universal setup for their VM. The blog says one can deploy an SP1 EVM verifier to any testnet and generate proofs on ~300k gas verification , implying the verifying key can be deployed easily. On Solana, we’d similarly rely on BN254 syscalls; SP1’s crate likely handles that.

**Performance:** SP1’s claim to fame is speed. It uses STARKs with recursion internally for fast proof generation, and only at the final step generates a small SNARK proof (Groth16) for L1 verification . They advertise *blazing fast* proof times due to their “precompile-centric architecture” accelerating hashing and curve ops . Benchmarks show SP1 outperforming other zkVMs by an order of magnitude in certain tasks . For our use case, which includes signature checks and simple arithmetic, SP1’s optimizations (like ed25519 precompile) are a huge boon – verifying an ed25519 signature might be thousands of R1CS constraints normally, but SP1 could do it in far fewer *virtual* cycles . This means proving a batch of bets with signatures might be practical in SP1, whereas in pure Arkworks it could be prohibitively slow.

That said, SP1’s absolute proving time is still likely in the order of seconds for non-trivial programs, but significantly less than naive approaches. Additionally, SP1 can leverage parallelization and even GPU acceleration (via Plonky3 maybe). For MVP, the difference might not be critical if our circuit is small, but if we aim to scale up (lots of bets, full verification), SP1 could handle it more gracefully.

**Verifier Setup on Solana:** SP1 proofs are Groth16 over BN254, so verifying them on Solana is fundamentally the same process as any Groth16. Succinct provides the sp1-solana crate which wraps the verification logic for proofs generated by SP1 . Under the hood, it likely uses the BN254 syscalls too. The key point is that the verifying key corresponds to the SP1 program (which might be generic for any SP1 VM execution of a given code). Possibly SP1 has a universal circuit and uses a universal setup – if so, the verifying key might be fixed size and known, making deployment easier. (Their Sui example indicates a Groth16 verifier over BN254 in Move, similar concept .) For Solana Anchor integration, we could either call the sp1-solana library directly in our program to verify (passing proof bytes and public inputs that the SP1 program expects), or we might generate a small verifier bytecode to include. Since sp1-solana likely provides a simple API, integration should be straightforward.

One consideration: SP1 proofs might have a slightly different structure of public inputs (maybe they just verify an output hash of the VM). We’d have to ensure the public inputs we get from SP1 align with the data we want to connect to Solana state. If not, we might adapt by embedding the state commitments in the SP1 program’s output.

**Comparing Speed & Dev Effort:**

- **Development Speed:** Arkworks requires careful constraint coding and debugging (which can be time-consuming), whereas SP1 allows writing and debugging the logic in a more natural way (you can even test the Rust code natively before proving). This means fewer opportunities for error in the circuit, and faster iteration.
- **Proving Speed:** For small circuits, Arkworks on a single CPU might be fine. But as complexity grows (especially if we push for signature verification), Arkworks proving could slow down a lot. SP1’s optimized VM might prove faster in those cases, maybe by 5-10x as they suggest for heavy crypto operations .
- **Trusted Setup:** Arkworks Groth16 requires a new trusted setup for each circuit (unless using PLONK or Marlin, but here we target Groth16 for fast verify). SP1’s approach still ends in Groth16; it likely also requires a one-time setup for the final SNARK (they might provide a pre-generated toxic waste disposal or use Plonk’s universal setup internally – need to confirm). The Succinct blog mentions no need for developers to worry about the cryptography – SP1 tooling handles it. They did mention auditing and preparing mainnet, implying trust assumptions are handled once by them . This can be seen as an advantage: using SP1, we leverage a well-tested proving system rather than crafting our own circuit from scratch.

**When to prefer Arkworks:** If the circuit is very tailored and small, and we want minimal dependencies, Arkworks is a good fit. Also, if we wanted to avoid relying on a newer project and stick to established libraries, Arkworks is more battle-tested in Ethereum circuits. We have full control over every constraint (which is both a blessing and a burden).

**When to prefer SP1:** If we anticipate frequent changes or extensions to the logic (like adding new game modes, more complex computations), SP1 would let us implement those quickly. Also, if verifying signatures or complex math is non-negotiable for security, SP1’s performance gain is crucial. SP1 is designed for rollups and zkEVM-like tasks , which is exactly our use case – a specialized rollup on Solana. It reduces the custom cryptography we need to do (we don’t have to implement pairing checks ourselves – the crate handles it).

**Hybrid approach:** We could even start with an Arkworks circuit (simpler, covering basics) and later migrate to SP1 for advanced features. The on-chain verifier would switch to using SP1’s verifying key then.

In conclusion, **Arkworks/Groth16** provides a direct, fine-tuned path with a bit more initial friction, while **SP1 zkVM** offers a higher-level, potentially faster development cycle for complex logic. For an MVP that doesn’t fully verify VRF or signatures, Arkworks might suffice since the circuit is straightforward. As we incorporate more verification (user sigs, VRF), SP1 becomes very appealing to manage the complexity and keep proving times low. Both produce Groth16 proofs on BN254 that can be verified on Solana, and both have supporting libraries (e.g. Lightprotocol’s groth16-solana for Arkworks/Circom proofs  and sp1-solana for SP1 proofs ). We may choose one based on which skillset is stronger on the team (low-level circuits vs. Rust VM programming). Given the long-term vision, investing in the SP1 approach might future-proof the system (especially if we later open-source it for others to run – SP1’s open-source zkVM could allow anyone to prove batches, not just our fixed circuit).

## **6. Key Rust Libraries and Resources**

Below is a list of Rust libraries and tools (with links) relevant to this project, spanning zero-knowledge, randomness, Solana development, and data availability:

- **Arkworks zkSNARK Ecosystem:** *Arkworks* is a collection of Rust libraries for constructing and using zkSNARKs . It includes ark-relations (for constraint systems), ark-snark (interfaces for SNARK proof systems) , concrete implementations like ark-groth16, and ark-ff for finite fields. Documentation and tutorials are available on the [Arkworks site and GitHub](https://arkworks.rs/). This is used if implementing custom Groth16 circuits.
- **Light Protocol Groth16 Verifier:** *groth16-solana* crate by Light Protocol – a Solana-oriented Groth16 proof verifier using BN254 curve precompiles . It provides utilities to prepare proofs and verifying keys for on-chain verification, and example code for calling the alt_bn128_pairing syscall. Repo: [Lightprotocol/groth16-solana on GitHub](https://github.com/Lightprotocol/groth16-solana).
- **Succinct SP1 zkVM:** *SP1* by Succinct Labs – a high-performance zkVM that outputs Groth16 proofs verifiable on many chains. The [SP1 documentation](https://docs.succinct.xyz/) and [GitHub project template](https://github.com/SuccinctLabs/sp1-starter) help in writing and proving programs. For Solana, the sp1-solana crate provides on-chain verification integration . SP1 allows using Rust (std, many crates) inside a provable context, with precompiled gadgets for hashing and signatures , which is very relevant to our use case.
- **Solana Anchor Framework:** *Anchor* is the leading framework for Solana smart contracts, simplifying development with Rust via macros and a runtime library . It handles account serialization, PDA derivations, and provides an *IDL* for clients. Docs: [Anchor Book](https://www.anchor-lang.com/) and [GitHub – project-serum/anchor](https://github.com/project-serum/anchor). We use Anchor for both Vault and Verifier programs to speed up development and ensure security (through account constraints and checks).
- **Solana SDK and Client Libraries:** The standard Solana Rust SDK (solana-program, solana-sdk) is used under the hood by Anchor. For the off-chain sequencer, the solana-client or Anchor client can be used to send transactions (for proof submission, etc.). Additionally, the *Solana Web3 JS* library might be used on the client side (not Rust, but worth mentioning for UI integration).
- **SPL Token SDK:** *Anchor SPL* and Solana Program Library for tokens – we use the anchor-spl crate which wraps token program CPI calls (for deposit/withdraw of USDC) . This provides convenient functions to transfer tokens, create associated accounts, etc., within the Anchor context.
- **Switchboard VRF:** *Switchboard* Oracle provides on-chain VRF randomness. While we don’t use it directly on-chain for bets, it’s a reference for VRF implementation. They have an [example Anchor integration](https://github.com/switchboard-xyz/vrf-cpi-example) and docs on [Switchboard VRF](https://docs.switchboard.xyz/randomness) . This can guide how to generate and verify VRF off-chain. (Switchboard’s approach: an off-chain oracle node produces an ed25519 signature as randomness and posts it to a VRF account, which on-chain clients can read .)
- **Orao VRF SDK:** *Orao Network’s VRF* is another Solana VRF provider . They offer the orao-solana-vrf Rust crate for requesting randomness in Anchor programs . We might not integrate it in MVP, but the crate shows how to format VRF requests and parse responses. Documentation on docs.rs: [orao_solana_vrf](https://docs.rs/orao-solana-vrf/latest/orao_solana_vrf/) .
- **ed25519-dalek:** A Rust library for Ed25519 signatures. We can use this off-chain (within the sequencer) to verify user signatures on bets or even to generate VRF-like outputs (ed25519-dalek can be used to sign a message and we treat the signature as random data). Dalek is well-known for fast, safe Ed25519. Link: [docs.rs ed25519-dalek](https://docs.rs/ed25519-dalek).
- **Rand and RNGs:** The Rust rand crate (and rand_chacha or rand_core) can be used for any general randomness needed (not for the actual coin flip outcome, which uses VRF, but perhaps for testing or simulating random bets).
- **Database Libraries:** For the sequencer’s state, we might use an embedded database. Candidates: *RocksDB* (via rust-rocksdb) for key-value storage, *PostgreSQL* via Diesel or SeaORM if we want a relational DB, or even an in-memory store for quick prototyping. These help store user balances and pending bets reliably.
- **Web Server and Async:** The sequencer stack can leverage frameworks like *Axum* or *Actix Web* for HTTP APIs, or *Tokio’s tungstenite* for WebSocket communications. For example, using Axum, we can easily set up an endpoint /place_bet that clients call, and a WebSocket channel to stream results back.
- **Surveillance/Explorer Tools:** We can use Solana Web3 or RPC directly to build an indexer for explorer. Libraries like *Helius RPC* (if using a third-party indexer) or *Solana-account-fetch* could assist in tracking program accounts. However, a simple approach is to use the official RPC in Rust or Node to get program logs and events.
- **Arweave Integration (Data Availability):** *Arloader* is a Rust CLI/library to upload data to Arweave efficiently . We could call Arweave’s HTTP API via Rust as well (there’s arweave-rs SDK ). Another option is *Bundlr*, a network for batching Arweave uploads (though official Rust support is limited). For our needs, Arweave’s standard REST API can be hit with Reqwest or similar. If using IPFS, Rust libraries like ipfs-api exist as well.
- **Merkle Tree Libraries:** If we later implement Merkle proofs for withdrawals, libraries like *Merkle Light* (merkle-light) or *Solana’s SPL Merkle proofs (used in Compression)* can be used to generate and verify Merkle paths. Noir or Circom could also be considered if we do more advanced circuits for Merkle inclusion proofs (but that’s beyond MVP).
- **Testing and Dev Tools:** Anchor comes with a testing framework (Mocha/TypeScript or Rust) to test programs. Additionally, *solana-test-validator* is used for local testing. For ZK, *cargo test* for the circuit and possibly *circom* (if using circom for rapid prototype). Also, *Criterion.rs* could be used to benchmark proving time in Rust.

Each of these tools will guide specific parts of the implementation. We will refer to their documentation as we integrate them to ensure we follow best practices (for example, Anchor’s constraints to prevent misuse of accounts, Arkworks tutorials for circuit design, and Succinct’s examples for SP1 usage).

## **7. Sequencer Stack Design (Rust Service, Real-time WS, Database, Recovery)**

The **Sequencer** is the off-chain engine of the coin flip game. It must be **low-latency**, reliable, and maintain consistency with on-chain state. Here’s the blueprint of its components and functionality:

- **API Layer (HTTP/WS):** The sequencer exposes an API for clients (players). A likely design is a REST endpoint for placing a bet and a WebSocket channel for streaming results. For instance:
    - POST /bet – the user provides their wallet address, the bet amount, and choice (heads/tails). Optionally, the request is signed by the user’s wallet (to authorize the bet off-chain). The sequencer immediately processes this and responds with an acknowledgement.
    - WebSocket bets – the user opens a WS connection. The sequencer pushes down the outcome of their bet as soon as it’s ready (within a second). This avoids the user having to poll.
    
    We can implement this in Rust using **Axum** (which supports WebSockets via Tungstenite) or **Actix-web** or even lower-level Tokio streams. Axum’s router would handle the HTTP post, and we’d manage a map of connected websocket clients to send results.
    
- **Bets Processing & Game Logic:** Upon receiving a bet:
    1. **Validate Input:** Check that the user is registered/have a vault, and that the bet amount is <= their off-chain available balance. If using signed requests, verify the signature with the user’s pubkey (e.g., using ed25519-dalek).
    2. **Lock Funds:** Deduct the bet amount from user’s available balance in the sequencer’s database to prevent double betting with the same funds. (These balances mirror on-chain deposits; the sequencer maintains its own copy since bets haven’t been settled on-chain yet.)
    3. **Generate Outcome:** Compute the random result (VRF or RNG). If using our VRF scheme, retrieve the next random outcome and associated proof. If not verifying VRF in-circuit, we can also simply use a CSPRNG here for now and store the seed/proof for transparency. However, using the VRF even off-chain means we have the proof if needed. This result is obtained in milliseconds.
    4. **Determine Win/Loss:** Compare outcome with user’s guess (or define outcome=heads means user wins, etc.) and calculate win amount. Prepare the pending balance update: if win, pending payout = +bet (user gain) from house; if lose, pending loss = -bet from user (house gain).
    5. **Send Result to User:** Through the WebSocket, send a message like { outcome: "heads", result: "win", payout: 2*bet } or similar. This is an optimistic real-time update – user now knows if they won or lost, before the on-chain confirmation.
    6. **Record the Bet:** Add an entry to an in-memory queue or list representing the current batch of bets being accumulated. Also store it in the database (with status “unsettled”) for durability. Each entry includes user id, bet amount, guess, outcome, perhaps a timestamp and an identifier.
- **Batching and Proof Generation:** The sequencer groups bets into batches to prove together. There are a few strategies:
    - **Time-based:** e.g. every X seconds (say 5 seconds) or every block (~400ms on Solana, but maybe 1-2s to gather multiple bets).
    - **Count-based:** e.g. batch when 50 bets are collected.
    - **Manual trigger:** for testing, maybe an operator command to force batch.
    
    For MVP, a time-based approach (every 5 seconds) is simple. When a batch triggers, the sequencer does the following:
    
    1. Freeze the current batch list and start a new list for incoming bets (to allow new bets while proving).
    2. Prepare the inputs for the SNARK. This means collecting the initial balances of each user in the batch and house, and the bet details (outcomes, amounts, etc.). The initial balances should match the last known on-chain (plus any bets from previous unsettled batches – essentially the off-chain “state” we track).
    3. Run the **ZK Prover** on this batch. If using Arkworks, this means constructing the Circuit with those bet inputs and calling Groth16::prove with the proving key . If using SP1, it means feeding the inputs into the SP1 program execution and getting a proof out. This is the heaviest step. We will do it in a background thread or task so as not to block new bets.
    4. Once proof is generated (say it takes 2-3s), the sequencer submits it to Solana (see next bullet). In terms of state, we now mark those bets as *pending settlement on-chain*. The balances remain updated in our off-chain view, but not yet final until chain confirms.
- **On-chain Submission & Retry:** The sequencer creates a Solana transaction calling the VerifyAndSettle instruction of the Verifier program. It assembles the required accounts: all user vault PDAs involved and the house vault PDA, plus the Verifier config. It attaches the proof and public inputs in the instruction data (Anchor can use a buffer or we might have to split it if too large for one instruction – Groth16 proof is ~256 bytes + overhead, which fits well within Solana TX limits of ~1232 bytes for instruction data). We then send this transaction to a Solana RPC node. We’ll use a robust method: maybe send via solana_client::RpcClient or Anchor’s Provider. We should specify skip_preflight: true (because preflight might not simulate well due to compute or program complexity) and max_retries for confirmation.
    
    If the transaction fails (e.g. proof didn’t verify or ran out of compute), the sequencer should catch that. We implement a **retry logic**:
    
    - If it’s a transient failure (rpc node issue, or a slot timing out), retry immediately a few times.
    - If it’s a verifiable failure (the program returned error, meaning proof invalid), this is serious – it implies a bug in our proving step or desync. The sequencer would log an alert. Potentially, it could fall back: maybe split the batch and try smaller, or last resort mark these bets as failed and not settled (and possibly refund users). Such failures should be extremely rare if the system is correct.
    
    Assuming success, we wait for confirmation (maybe wait for 1 block confirmation, since it’s final in Solana after a block due to optimistic confirmation). Once confirmed, we mark those bets in DB as **settled** and update the off-chain state:
    
    - Decrease the house’s off-chain balance if it paid out net, or increase if net gain.
    - Mark users’ off-chain balances equal to on-chain now (which our off-chain had already been tracking; ideally they match).
    - These off-chain balances should now equal the actual on-chain vault balances (we might cross-verify by reading the vault accounts after settlement to ensure consistency).
    
    The sequencer can then broadcast maybe via WebSocket or some channel that batch X was confirmed. Individual users might not need this notification because they already got their result, but it might be used to update a UI indicator (“settled on chain”).
    
- **Database and State Recovery:** A lightweight database (like RocksDB or SQLite or Postgres) is used to store:
    - User balances and vault info,
    - Pending bet records (with batch id, status),
    - Maybe the last processed batch number or state root.
    
    This ensures that if the sequencer restarts (crash or upgrade), it can recover the latest known off-chain state:
    
    - It can reload all unsettled bets and maybe re-run proof if needed, or at least know what’s outstanding.
    - It can query the chain for the latest settled batch or vault balances to reconcile any difference.
    
    Upon restart, the sequencer should do a **resync routine**: check the Verifier program’s state (like a last batch counter or look for the latest verified batch transaction) to ensure it didn’t miss a submission. Then compare to its DB:
    
    - If it finds a batch in DB marked pending but already on-chain, it finalizes it.
    - If a batch is partially proved but not sent, it can attempt to resend.
    - If some bets were acknowledged to users but not yet in any batch (say a crash happened before batching), those would still be in DB as not batched; the sequencer can include them in a new batch so they’re not lost.
- **Concurrency & Ordering:** Because bets come in while proving is happening, we have to ensure thread safety. Likely we’ll use an async runtime (Tokio). We can protect shared state (like the list of current bets being filled) with a mutex or channel:
    - One approach: have a single task loop that every T seconds takes the current batch list, moves it to a separate object for proving, and clears the main list. This can be done with a mutex lock on the list.
    - Another approach: use a message queue (channel). Each bet request is a message; a separate worker aggregates messages into batches. But simpler is fine for MVP.
    
    We also ensure the on-chain submission is done sequentially (don’t send two overlapping batches that might conflict on the same vault accounts). We wait for batch N to be confirmed (or at least submitted) before sending N+1 to avoid race conditions on chain state. This is important: if a second batch includes one of the same user’s vault, it must use the updated balance from batch N, so it cannot be processed until N is applied. Thus, batches must be strictly ordered. We can enforce this by always waiting for confirmation of batch k before proving batch k+1 (or we can prove in parallel but not submit until previous is done and we’ve updated initial states accordingly).
    
- **Logging and Monitoring:** The sequencer should log all key events: bets received, outcomes, batch creation, proof time, submission TX id, success or failure, etc. These logs (and metrics) will help tune performance. We may include a simple metrics endpoint (prometheus or just console logs) for number of bets per second, average latency from bet to result, and so on.
- **State Channel vs On-chain Fallback:** In case the on-chain proof verification is down (say Solana outage or our program bug), the sequencer could pause accepting new bets and/or allow users to withdraw the old-fashioned way (via an admin unlocking vaults). Communication to users about system status might be needed in production, but for MVP we assume happy path.

Overall, the sequencer is like a **Layer-2 coordinator**. It ensures a smooth user experience (fast results) while batching the “heavy” trust-minimized operations (ZK proofs & L1 updates). In this design, users trust the sequencer temporarily, but final reconciliation is trustless on L1. We emphasize robust error handling – e.g., if proof generation fails an assertion, the sequencer might split the batch and try smaller pieces, or at least notifies devs. If a transaction to L1 fails due to a full block or network issue, it retries until it goes through (with exponential backoff perhaps). The database’s presence means even if the process crashes after sending proof but before marking bets settled, on restart it can see that the chain state (maybe by reading vault balances) has moved and mark them settled, or re-submit if it hadn’t succeeded.

We will implement the sequencer in Rust for performance and reliability (leveraging crates like tokio for async, and possibly serde_json for any JSON API work, etc.). This also allows us to reuse data structures between on-chain and off-chain (for example, define a Bet struct or an outcome enum in a shared module for consistency).

## **8. Testing & Performance Considerations**

Testing the system requires both on-chain program tests and off-chain integration tests, including performance measurements. We outline a multi-step testing strategy:

### **Anchor Program Testing (Local Validator):**

Using Anchor’s test framework, we can write unit tests for the Vault and Verifier instructions:

- **Vault operations tests:** Start a local Solana test validator and deploy the Vault program. Test deposit and withdrawal flows:
    1. User A deposits X SOL -> check Vault PDA lamports increased .
    2. User A deposits Y USDC -> check Vault’s ATA balance and Vault account’s token_balance updated .
    3. Try withdrawing more than balance -> expect error.
    4. If any lock/timelock introduced (not in MVP), test those conditions as in the example .
- **Verifier logic tests:** We can simulate a trivial proof for testing before actual integration:
    - Write a dummy circuit (or use a hardcoded proof) that always returns true for specific inputs. Use that to test the VerifyAndSettle instruction path. For example, temporarily modify the program to skip actual verification and just update balances given some dummy “proof” input, and ensure balances update correctly and only when called by authorized actor.
    - Once the real proving is integrated, in testing we can generate a proof on-the-fly in the test (Anchor tests allow calling Rust code) and feed it into the transaction. This requires having the proving key available in the test. We could pre-generate a test proof (like a proof that 2+2=4) and include its VK in program for testing.
    - Another approach: treat the ZK verification as external and assume it’s correct, focusing on “if proof valid then state changes”. Full end-to-end proof verification can be tested once we integrate off-chain proving.
- **Edge cases:** Test what happens if VerifyAndSettle is given mismatched accounts or wrong balances (should fail proof or explicit checks). Also test multiple users in one batch scenario (if possible to simulate in one transaction – maybe by crafting a proof for multiple accounts).

### **ZK Circuit Testing:**

For the circuit (Arkworks or SP1), we write separate tests:

- **Pure circuit test:** Given some artificial inputs (e.g. User balance 100, bet 10 heads -> expect win), generate a proof and verify it *off-chain* using the Arkworks verifier (which we trust for testing). We can embed assertions like: if user didn’t have enough balance, the circuit should not satisfy constraints (we might not catch that without a full proof check, but we can craft a scenario to see it fail).
- **Invalid case testing:** Try to prove an invalid state (e.g., user gains money without house losing) and ensure the proof generation fails or the verifier (off-chain) rejects it. This ensures our constraints cover that scenario.
- If using Arkworks, we may utilize the ark-groth16::verify on the proof to ensure it returns true for valid proofs and false for tampered proofs .
- If using SP1, we test the Rust program natively for known cases and also run the SP1 prover to see that a proof verifies with the SP1 SDK’s verifier.
- **Performance test (local):** We can measure how long proving takes for a given batch size on a single CPU (we might do this in a cargo test or a separate benchmark binary). This helps adjust batch sizes. For example, if proving 50 bets with signatures takes 5 seconds, maybe we reduce batch or drop sig verification.

### **Integration Test (End-to-End):**

Simulate the full flow on a local Solana network:

- Use the **solana-test-validator** (perhaps via Anchor’s #[tokio::test] or a standalone environment) and run the sequencer as a thread or process.
- The test would:
    1. Set up two users (AirDrop them SOL for fees, create their vault PDAs via deposit).
    2. Run the sequencer connected to this local validator (point RPC to localhost).
    3. Have the users send a couple of bet requests to the sequencer (this can be done by directly calling the sequencer’s Rust functions if in-process, or HTTP calls if out-of-process).
    4. Wait a short time for sequencer to return results and submit proof.
    5. Verify on-chain state: query the vault PDAs to confirm balances reflect the bets’ outcomes correctly. Also ensure the house vault changed appropriately.
    6. If possible, inspect that the Verifier program’s logs included a “Proof is valid!” message or similar, indicating the proof verification succeeded.
    7. Check that a withdrawal works after a bet (e.g. user withdraws some of their winnings successfully).

This test ensures the coordination between off-chain and on-chain is correct. We’d also simulate failure scenarios:

- E.g., deliberately feed a wrong proof (maybe flip one byte) and confirm the on-chain program rejects it (transaction error). The sequencer should detect that and not mark bets settled.
- Perhaps stop the sequencer after sending a proof but before acknowledging to itself, and restart it to test recovery.

### **Load and Latency Testing:**

To ensure sub-second response, we test the sequencer under load:

- Simulate say 100 concurrent bet requests (we can spawn tokio tasks or threads making requests).
- Measure the time from request to receiving result for each. It should be well under 1s on average. We identify bottlenecks if any (maybe the VRF generation or database writes).
- Also measure the time from bet to on-chain settlement in these tests (though that might be in seconds due to batching interval).
- We can gradually increase number of bets per batch and see how that affects proving time and overall throughput. For example, find the max batch size that can still be proven in under, say, 10 seconds on commodity hardware, to inform production settings.

### **Security Testing:**

- Attempt to place bets that break rules: bet more than deposited, or withdraw mid-batch (should either be disallowed or handled).
- If a user tries to use the same vault from two sequencers (not applicable in our design, since one operator).
- Ensure signatures (if used) are checked: test with a wrong signature and see that the sequencer rejects the bet.

Given it’s an MVP, formal verification is out of scope, but we rely on tests to catch logic errors. We will also keep an eye on **Solana compute limits** in test: after deploying on localnet, call a VerifyAndSettle with a large proof to see how close to the 200k CU we get. The Light Protocol crate said <200k CU for Groth16 verify , so we should be fine, but if we overload public inputs or additional checks (like iterating accounts), we need to ensure we stay within ~1.4M CU (the max in a Solana transaction currently, though typical target is <800k for comfort). We might see actual compute usage via tools or by observing logs.

Finally, we will write a **Checklist** (below) which will serve as a plan for implementing and testing in sequence.

## **9. Implementation Task Checklist**

Here is an ordered list of tasks to implement the MVP, with an estimated difficulty for each (★☆☆☆ = easy, ★★★★ = hard):

1. **Anchor Program Setup** – *Scaffold the project structure.* Use Anchor to create two programs (or modules): Vault and Verifier. Define IDLs. *Difficulty:* ★☆☆☆ (basic Anchor setup).
2. **Vault Account Schema** – Define the Vault struct (fields for balances) and write the Anchor account attribute for PDA (seeds = [b”vault”, user.key]). Also create a global House vault account. *Difficulty:* ★☆☆☆.
3. **Vault Instructions (Deposit/Withdraw)** – Implement deposit_sol, withdraw_sol, deposit_token, withdraw_token. Use Anchor constraints to ensure correct accounts (e.g., vault.owner == user.key for withdraw) and invoke system/token programs for transfers . *Difficulty:* ★★☆☆ (must handle invoke_signed for vault PDA).
4. **Verifier Account & Instruction** – Create VerifierConfig account and a bare-bones verify_and_settle instruction handler. Initially, this handler can be a stub that simply checks an “admin” signature and prints a log. *Difficulty:* ★☆☆☆ (just plumbing).
5. **ZK Circuit (Prototype)** – Implement a simple circuit that takes one bet and updates balances. For start, maybe even a circuit that checks something trivial (like 1+1=2) to test proving pipeline. *Difficulty:* ★★☆☆ (familiarizing with Arkworks or SP1 toolchain).
6. **Generate Trusted Setup & Keys** – Run a one-time Groth16 setup for the circuit to obtain proving and verifying keys . *Difficulty:* ★☆☆☆ (just running code, but requires careful handling of toxic waste in real scenario).
7. **Integrate Verifying Key On-Chain** – Hardcode or store the verifying key in the program. Write the logic in verify_and_settle to perform pairing checks via groth16_solana crate . Test this with a known valid proof (maybe from a test vector). *Difficulty:* ★★★☆ (dealing with byte endianess and syscalls is tricky).
8. **Expand Circuit to Batch** – Extend the circuit to handle multiple bets and multiple accounts. Verify off-chain that it works for a couple of scenarios. This is one of the most challenging tasks, as it involves many constraints. *Difficulty:* ★★★★ (complex circuit logic).
9. **Sequencer Service** – Set up a basic Rust application for the sequencer. Implement data models (User, Bet). Incorporate a simple in-memory or file-based DB (even JSON or SQLite to start). *Difficulty:* ★★☆☆ (typical Rust server coding).
10. **Real-Time API** – Implement the REST/WS endpoints for placing bets and sending results. Test locally by simulating a bet and ensuring the response comes back quickly. *Difficulty:* ★★☆☆.
11. **Randomness (VRF) Integration** – Generate a VRF keypair for the sequencer. Use ed25519-dalek to sign a message (like a counter or random seed) to produce randomness. Store the signature as proof. (We might skip full VRF proof verification but at least have the mechanism in place.) *Difficulty:* ★★☆☆.
12. **Batch Assembly** – Implement logic to gather bets into batches. Possibly use a timer (Tokio interval) to trigger batch finalization. Mark bets as being processed in that batch. *Difficulty:* ★★☆☆.
13. **Prover Integration** – Connect the ZK circuit with the sequencer. For each batch, feed the inputs to the circuit (or SP1 VM) and generate a proof. This involves calling our proving function from within the sequencer’s runtime (which might be CPU intensive – consider using a dedicated thread or task with low priority for proving to not block the WS event loop). *Difficulty:* ★★★☆ (ensuring async doesn’t conflict with heavy compute).
14. **Submit Proof TX** – Use Solana RPC (probably via solana_client crate) to send the proof to the Verifier program’s verify_and_settle. Constructing the TX with all accounts and proof bytes correctly is a bit involved (we might generate the instruction data by hand or extend Anchor client to support it). *Difficulty:* ★★★☆ (handling large tx data, ensuring account indices align with program expectations).
15. **Confirm and Update** – After sending the tx, wait for confirmation. On success, update the off-chain balances officially (though they were already optimistically updated). On failure, handle according to the plan (retry or abort). *Difficulty:* ★★☆☆.
16. **Withdraw Flows** – Test that a user can call the on-chain withdraw after settlement and get their funds. This might involve writing a client script using Anchor to call withdraw. *Difficulty:* ★☆☆☆.
17. **Explorer/Monitoring Tool** – (Optional for MVP) Develop a simple script or web page that reads events. Could subscribe to the program logs via WebSocket (Solana RPC pubsub) or parse the transaction logs. This tool would help during testing to see what’s happening in near real-time. *Difficulty:* ★☆☆☆.
18. **End-to-End Test & Debug** – Run everything together on devnet or localnet with a realistic scenario. Likely many small bugs will appear (timing issues, account not found, etc.). Fix these iteratively. *Difficulty:* ★★★☆ (integration debugging often unexpected).
19. **Security Review & Cleanup** – Do a pass to ensure all Anchor constraints are set (e.g., check that vault PDA’s owner is our program in verify_and_settle, to avoid someone passing in a fake account), that no leftover print statements or dev keys remain. Also ensure secret keys (VRF key) are handled safely (not logged). *Difficulty:* ★★☆☆.
20. **Documentation & Deployment Scripts** – Write a README for the repository, explaining how to run the sequencer, how to deploy the programs, how to perform the trusted setup (so others can reproduce if needed). Create scripts for deployment (Anchor provides some, but customizing maybe for setting up verifier config etc.). *Difficulty:* ★☆☆☆.

This sequence is roughly in order of implementation. Some tasks can be done in parallel by different team members (e.g., one works on circuit while another does vault program). The most challenging parts are the ZK circuit and integrating it with Solana – expect to allocate significant time for those (hence ★★★★ for circuit, ★★★ for on-chain verify integration).

## **10. Optional Extensions and Future Improvements**

Finally, once the core MVP is functional, we have a roadmap of enhancements that can increase the system’s security, functionality, and decentralization:

- **Trustless Withdrawals via Merkle Proofs:** In the fully realized rollup design, users could withdraw funds from L1 without the operator directly updating their vault account. Instead, the operator would maintain a Merkle tree of all user balances off-chain. Each batch proof would include the new Merkle root . The on-chain state would just keep the latest root (and perhaps a time when it was updated). A user withdraw would involve providing a Merkle proof of their balance in that root and a signature from them to burn that balance. The program would verify the Merkle proof (either via another zk proof or using an on-chain Merkle verification which might be expensive without ZK compression). This approach enables the rollup to scale to many users without storing each one on-chain, and allows users to exit independently if the operator disappears (as long as they have the latest state). Implementing this would require adding Merkle constraint logic in the circuit or using a recursive proof to compress many Merkle verifications into one . It’s complex, but it is the ultimate goal for a “validium” style rollup on Solana.
- **Multi-Token Support:** Currently we considered SOL and one SPL token. Extending to multiple SPL tokens (USDT, etc.) is straightforward: the Vault account could either have multiple balance fields or we create separate vault PDAs per token (e.g. seeds [user, token_mint]). The latter might be more dynamic. The circuit and proof would then need to handle multiple asset types in state. Alternatively, each token could be a separate instance of the game (with its own batches). Multi-currency support would attract more users. This would involve updating deposit/withdraw instructions to accept a mint and maybe using token-2022 for associated account PDAs if needed.
- **Multiple Games or Bet Types:** We could generalize beyond coin flip (e.g. other simple games like dice roll, or higher payout odds) using the same vault system and sequencer. The circuit could incorporate a game ID or logic branch. Or separate circuits for separate games. This is more of a product extension.
- **Multi-Operator / Decentralized Sequencer:** To remove the central trust and single point of failure, we can consider multiple sequencers or a rotating leader. Multi-operator implies:
    - The need for an on-chain consensus or a mechanism to decide whose proof to accept for a given batch range. Possibly an approach like having each operator bonded and slashable if they don’t follow some rules, or using a round-robin where each has a slot to submit a proof.
    - The circuit could be extended to include a signature by the operator on the batch, and the on-chain program could require that the operator is the expected one for that batch (ensuring one operator can’t usurp another’s turn).
    - Data availability becomes crucial with multi-operators, since any operator should be able to take over and continue if one fails – thus state must be published trustlessly. Using an external DA (like a Celestia cluster) or storing the transactions on-chain might be needed.
    - This extension basically moves toward a fully decentralized rollup (where maybe validators collectively decide outcomes). It’s a complex topic and likely beyond MVP without serious redesign. But designing from day one with clean interfaces (like the proof just proves validity, doesn’t matter who provided it) helps. We did consider an “admin key” for verify_and_settle; that could later be replaced by a permissionless verify if we’re confident fraudulent proofs can’t be accepted (they can’t if SNARK soundness holds).
- **House Edge and Revenue**: Add a configurable fee or edge for the house on each bet (e.g. win pays 1.98x instead of 2x, 2% edge to house). This would reflect in the state updates (house doesn’t lose the full bet amount on win, etc.). The circuit and program would need to incorporate the fee logic (ensuring the math is correct). This is a business feature but good to plan. Similarly, a referral or dividend system could be built on top.
- **Continuous Randomness (VDF/Chain VRF):** Instead of relying solely on the sequencer’s VRF, we could incorporate Solana’s upcoming native randomness or a verifiable delay function to add unpredictability that even the sequencer can’t fully control. For example, using the Solana **recent block hash** combined with user-supplied randomness could be an idea (although blockhash is not reliably random, if user adds one secret, it becomes commit-reveal essentially). Optional enhancements here could improve fairness.
- **Comprehensive Explorer & User Interface:** Build a nice UI with wallet integration so users can deposit, then place bets through the sequencer’s API (perhaps via a web UI that connects wallet for deposit and uses a normal web socket for the game). The explorer part can be integrated into this – showing history of bets, proofs, etc. This isn’t protocol-level, but important for usability.
- **Audit and Formal Verification:** As a future improvement, especially if the project handles significant value, a formal audit of the smart contracts (to ensure no bug can drain vaults) and a security review of the circuit (to ensure no constraint omission) is essential. Additionally, performance tuning: using GPU for proving (especially if using SP1 or if large batches) could be explored, as Succinct mentions AVX-512 and GPU in their benchmarks .

Each of these extensions can be added incrementally. Our MVP’s architecture is designed to not preclude them:

- The vault model can handle multiple tokens.
- The verifier can switch out the SNARK circuit without changing the overall program logic (just update verifying key and constraints).
- Multi-operator might require an upgrade to how verify instruction works (maybe accepting proofs from any and having a check on a global state sequence to prevent conflicts).

In summary, the blueprint establishes a solid, scalable foundation. We start with a centralized sequencer and a SNARK-verified batch update (a **validity rollup** approach) on Solana. From there, we can iterate towards a production-ready system with decentralized operation, better user economics, and possibly generalize it to other fast betting games.